#!/usr/bin/env python3
"""
ë‚´ ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ìì£¼ ì¸ìš©í•˜ì§€ë§Œ ì•„ì§ ì—†ëŠ” ë…¼ë¬¸ë“¤ ì°¾ê¸°
"""

import json
import time
import requests
from collections import Counter
from pathlib import Path

BASE_URL = "https://api.semanticscholar.org/graph/v1"


def get_paper_details(paper_id: str) -> dict:
    """S2 IDë¡œ ë…¼ë¬¸ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
    url = f"{BASE_URL}/paper/{paper_id}"
    params = {"fields": "title,authors,year,citationCount,venue,url"}

    try:
        resp = requests.get(url, params=params, timeout=10)
        if resp.status_code == 200:
            return resp.json()
        elif resp.status_code == 429:
            print("  Rate limited, waiting...")
            time.sleep(60)
            return get_paper_details(paper_id)
    except Exception as e:
        print(f"  Error: {e}")
    return None


def main():
    # papers.json ë¡œë“œ
    with open("papers.json", "r", encoding="utf-8") as f:
        data = json.load(f)

    papers = data.get("papers", data)

    # ë‚´ ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ìˆëŠ” S2 IDë“¤
    my_s2_ids = set(p.get("s2_id", "") for p in papers if p.get("s2_id"))
    print(f"ë‚´ ë¼ì´ë¸ŒëŸ¬ë¦¬: {len(my_s2_ids)}ê°œ ë…¼ë¬¸ (S2 ID ìˆìŒ)")

    # ëª¨ë“  references ìˆ˜ì§‘ (ë‚´ ë…¼ë¬¸ë“¤ì´ ì¸ìš©í•œ ê²ƒë“¤)
    all_refs = []
    for p in papers:
        refs = p.get("references", [])
        all_refs.extend(refs)

    print(f"ì´ {len(all_refs)}ê°œ references ë°œê²¬")

    # ë‚´ ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ì—†ëŠ” ê²ƒë“¤ë§Œ í•„í„°ë§
    missing_refs = [r for r in all_refs if r not in my_s2_ids]
    print(f"ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ì—†ëŠ” ê²ƒ: {len(missing_refs)}ê°œ")

    # ê°€ì¥ ë§ì´ ì¸ìš©ëœ ìˆœìœ¼ë¡œ ì •ë ¬
    ref_counts = Counter(missing_refs)
    top_missing = ref_counts.most_common(30)

    print(f"\n{'='*60}")
    print("ğŸ“š ê°€ì ¸ì™€ì•¼ í•  ë…¼ë¬¸ TOP 30 (ë‚´ ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ìì£¼ ì¸ìš©)")
    print(f"{'='*60}\n")

    results = []
    for i, (s2_id, count) in enumerate(top_missing, 1):
        print(f"[{i}/30] Fetching {s2_id[:20]}... (cited by {count} papers)")

        details = get_paper_details(s2_id)
        if details:
            title = details.get("title", "Unknown")
            year = details.get("year", "N/A")
            citations = details.get("citationCount", 0)
            venue = details.get("venue", "")
            url = details.get("url", "")
            authors = details.get("authors", [])
            first_author = authors[0]["name"] if authors else "Unknown"

            result = {
                "rank": i,
                "cited_by_my_papers": count,
                "title": title,
                "first_author": first_author,
                "year": year,
                "venue": venue,
                "global_citations": citations,
                "url": url,
                "s2_id": s2_id
            }
            results.append(result)

            print(f"   {title[:60]}...")
            print(f"   {first_author} ({year}) - Cited: {citations}")
            print()

        time.sleep(1)  # Rate limiting

    # ê²°ê³¼ ì €ì¥
    output_path = Path("missing_papers.json")
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… ê²°ê³¼ ì €ì¥: {output_path}")

    # ìš”ì•½ ì¶œë ¥
    print(f"\n{'='*60}")
    print("ğŸ“‹ ìš”ì•½: ê°€ì ¸ì˜¬ ë…¼ë¬¸ TOP 10")
    print(f"{'='*60}")
    for r in results[:10]:
        print(f"{r['rank']:2}. [{r['cited_by_my_papers']}íšŒ ì¸ìš©] {r['title'][:50]}...")
        print(f"    {r['first_author']} ({r['year']}) | Global: {r['global_citations']} citations")


if __name__ == "__main__":
    main()
